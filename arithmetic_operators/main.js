// 1. Add x and y, and print the result to the console.
let x = 10;
let y = 15;
console.log(x + y);
// 2. Subtract y from x, and print the result to the console. Then, subtract x from y and print the result to the console.
console.log(x - y);
console.log(y - x);
// 3. Multiply x and y, and print the result to the console.
console.log(x * y);
// 4. Divide x and y, and print the result to the console. 
console.log(x / y);
// 5. Declare another variable "z" with the value "10". Multiply x and y. Then, divide the result by z. Store the result in a new variable named "resultOne". Print "resultOne" to the console.
let z = 10;
let resultOne = (x * y) / z;
console.log(resultOne);
// 5. Declare two variables "a" with the value of 15 and "b" with the value of 9. Print the remainder when a is divided by b. 
let a = 15;
let b = 9;
console.log(a % b);
// 6. Declare another variable "c" with the value of 20. Add a and b, then multiply the result by c. Store the result in variable "resultTwo".  Print "resultTwo" to the console.
let c = 20;
let resultTwo = (a + b) * c;
console.log(resultTwo);
// 7. Increment a. Print the result to the console.
a = ++a;
console.log(a);
// 8. Decrement b. Print the result to the console.
b = --b;
console.log(b);
// 9. Subtract a from b and store this in a new variable "d". Add d and c. Print the result to the console.
let d = b - a;
console.log(d + c);
// 10. Print the remainder when "resultOne" is divided by "resultTwo" to the console. 
console.log(resultOne % resultTwo);
 
